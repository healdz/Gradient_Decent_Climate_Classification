{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Experiment Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Overview: This project will read in and examine the sentiment of tweets related to global warming with the intent of understanding the tweets author's opinions towards the existence of climate change. With the growing threat of climate change it can be suprising that there is still a significant amount portion of the population who doubt or diminish the significance of the global trend and this data set offers insite into identifying the two groups with tweets classified into yes (climate change exists), no (climate change does not exist), and NaN (the tweet simply gives information about climate change (no opinion or sentiment). For the purposes of this sentiment analysis project the subjective tweets will be the main focus of the project as we attempt to train a model to classify whether or not a person believes in climate change based on a tweet.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) The Data:  The data was downloaded from a public repository on data.world and consists of the tweet itself, the determination of belief column titles 'existence' (climate change exists, does not exist, or tweet does not contain an opinion (conveying information)), and a confidence evaluation of the existence column that gives the authors relative confidence on the assesment of sentiment made in the existence column. For the puposes of this project only the tweets that were determined as conveying sentiment will be used and the confidence of that prediction will be ignored (the existence column will be taken as base truth). It is important to note that this data is relatively old having been published in 2013 however, while the amount of climate change disbelievers has decreased since then, it is still relevant in todays society. Thus, creating a model based off older tweets can still be applicable to data collected today and future data collected about coimate change beliefs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Ethical Considerations: This data is public in both its collection from twitter and from the source on data.world which simply serves the pupose of aggregating otherwise widespread information that would be difficult to collect for public use from Twitter. The only potential source of ethical consideration in regards to this data set is that Twitters API for data collection maintains rules and regulations as well as an approval process to help prevent the inappropriate mining or use of mined data. Since, this data has already been collected however, we know that it was collected from twitter under those guidleines by the initial authors and since the scope of this project is not controversial in nature nor outside the intended use of the original authors it again maintains the standards of ethical data use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Load the data from the downloaded csv (link in works cited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_warming = pd.read_csv('global_warming_sentiment.csv',encoding= 'unicode_escape')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "      <th>existence.confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>URUGUAY: Tools Needed for Those Most Vulnerabl...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet existence  \\\n",
       "0  Global warming report urges governments to act...       Yes   \n",
       "1  Fighting poverty and global warming in Africa ...       Yes   \n",
       "2  Carbon offsets: How a Vatican forest failed to...       Yes   \n",
       "3  Carbon offsets: How a Vatican forest failed to...       Yes   \n",
       "4  URUGUAY: Tools Needed for Those Most Vulnerabl...       Yes   \n",
       "\n",
       "   existence.confidence  \n",
       "0                1.0000  \n",
       "1                1.0000  \n",
       "2                0.8786  \n",
       "3                1.0000  \n",
       "4                0.8087  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_warming.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>URUGUAY: Tools Needed for Those Most Vulnerabl...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet existence\n",
       "0  Global warming report urges governments to act...       Yes\n",
       "1  Fighting poverty and global warming in Africa ...       Yes\n",
       "2  Carbon offsets: How a Vatican forest failed to...       Yes\n",
       "3  Carbon offsets: How a Vatican forest failed to...       Yes\n",
       "4  URUGUAY: Tools Needed for Those Most Vulnerabl...       Yes"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop Existence.Confidence column (not needed)\n",
    "global_warming.drop(['existence.confidence'],axis =1, inplace =True)\n",
    "global_warming.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes' nan 'No' 'Y' 'N']\n",
      "['Yes' nan 'No']\n",
      "['Yes' 'No']\n",
      "[1 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>URUGUAY: Tools Needed for Those Most Vulnerabl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  existence\n",
       "0  Global warming report urges governments to act...          1\n",
       "1  Fighting poverty and global warming in Africa ...          1\n",
       "2  Carbon offsets: How a Vatican forest failed to...          1\n",
       "3  Carbon offsets: How a Vatican forest failed to...          1\n",
       "4  URUGUAY: Tools Needed for Those Most Vulnerabl...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the existence column such that only binary yes / no is left represented as 1 / 0 respectively\n",
    "print(global_warming['existence'].unique())\n",
    "global_warming = global_warming.replace(['Y','N'],['Yes','No'])\n",
    "print(global_warming['existence'].unique())\n",
    "global_warming = global_warming.dropna()\n",
    "print(global_warming['existence'].unique())\n",
    "global_warming = global_warming.replace(['Yes','No'],[1,0]) #convert yes to 1 and no to 0\n",
    "print(global_warming['existence'].unique())\n",
    "global_warming.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet        object\n",
       "existence     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check types to make sure everything is set as needed for model creation\n",
    "global_warming.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Define the functions to clean the text data, tokenize the document, and construct a TfidfVectorizer (these will be used in optimizing the SGD function in the next section as part of the grid search optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional functions for data cleaning and tokenizing\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    porter = PorterStemmer()\n",
    "    line = [porter.stem(word) for word in text.split()]\n",
    "    return line\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Potential functions for tfidf / hashing vector creation\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test data\n",
    "\n",
    "X = global_warming['tweet']\n",
    "y = global_warming['existence']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X, y, \n",
    "                     test_size=0.2, \n",
    "                     random_state=0, \n",
    "                     stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Model Optimization and Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Define the model and optimize it using a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Initial Model\n",
    "model = SGDClassifier(loss='log', random_state=1)\n",
    "SGD_vect = Pipeline([('vect', vect),\n",
    "                     ('clf', model)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vect', 'clf', 'vect__alternate_sign', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__n_features', 'vect__ngram_range', 'vect__norm', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'clf__alpha', 'clf__average', 'clf__class_weight', 'clf__early_stopping', 'clf__epsilon', 'clf__eta0', 'clf__fit_intercept', 'clf__l1_ratio', 'clf__learning_rate', 'clf__loss', 'clf__max_iter', 'clf__n_iter_no_change', 'clf__n_jobs', 'clf__penalty', 'clf__power_t', 'clf__random_state', 'clf__shuffle', 'clf__tol', 'clf__validation_fraction', 'clf__verbose', 'clf__warm_start'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD_vect.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Parameters for tunig the initial model\n",
    "stop = stopwords.words('english')\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__preprocessor':[None,preprocessor],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__stop_words':[stop,None],\n",
    "               'clf__loss' : ['log', 'modified_huber'], #only trying loss functions that support probability estimators\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__alpha': [.00001,.0001,.01,.1,1.0]}]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  4.5min finished\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8289940828402367\n",
      "{'clf__alpha': 1e-05, 'clf__loss': 'log', 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__preprocessor': <function preprocessor at 0x1253df430>, 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], 'vect__tokenizer': <function tokenizer_porter at 0x1253df550>}\n"
     ]
    }
   ],
   "source": [
    "#use a grid search to optimize the model\n",
    "gs_SGD = GridSearchCV(SGD_vect, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "gs = gs_SGD.fit(X_train, y_train)\n",
    "print(gs_SGD.best_score_)\n",
    "print(gs_SGD.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834319526627219\n"
     ]
    }
   ],
   "source": [
    "predicted = gs.predict(X_test)\n",
    "acc = accuracy_score(y_test, predicted)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Outcomes of initial fit of the model\n",
    "\n",
    "### The above model was fitted using cross validation on grid search optimization of the model. The model itself represents a hashing vector applied to the data which is then processed via an SGD classifier. In doing so the grid search selected a hasing vector that was optimized to tokenize the data via the tokenizer_porter which both tokenizes each line and stems the words extracted as tokens to their root (via the porter method). Prior to tokenizing the hashing vector preprocessed the data to remove punctuation with the exception of emojis as defined in the function 'preprocessor' above and chose to remove stop words via the nltk.corpus stop_words package. With the data vectorized using that optimized hashing function the SGD classifier selected an l2 penalty, an alpha value of .00001 and a log loss function as the optimized parameters. With this optimization the model produces an accuracy of 82.899% on the training data which only slightly improved to an accuracy of 83.432% when applied to the test data. These selected parameters will be used in the following code to create a model that can be used in an online web application designed to continue training the data in the hopes of imporving its accuracy.\n",
    "\n",
    "### As a note on the project requirments the SGD classifier differes from the LR classifier used in Ch08 and similarly the HashingVectorized optimized here is different than the TFIDF used in the book such that this model can be applied to an online format with updates via partial_fit as new information comes in via the web application. In addition to these obvious differences between the book and this project the hashing vector includes the preprocessor parameter to be optimized and the SGD is being optimized by analyzing the loss function while the book did not include either of these as options for the grid search. By doing so both have been updated from their defaults in order to improve the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) FIt the model against the entire data set and pickle the model for use in web application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redifine functions based on the optimized LR above converting the LR to a SGD classifier to accomodate online learning and the TFIDF vectorizor to a hashing vecotrizer to accomodate online learning\n",
    "\n",
    "vect_optimized = HashingVectorizer(decode_error='ignore',\n",
    "                         norm = None,\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=preprocessor,\n",
    "                         stop_words = stop,\n",
    "                         tokenizer=tokenizer_porter)\n",
    "                           \n",
    "new_model = SGDClassifier(loss='log', random_state=1,alpha = .00001, penalty = 'l2') \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "# # test the new models created to accomodate online learning for comparison to optimized LR model\n",
    "X_train_vect = vect_optimized.transform(X_train)\n",
    "\n",
    "new_model.partial_fit(X_train_vect,y_train,classes=(0,1))\n",
    "\n",
    "X_test_vect = vect_optimized.transform(X_test)\n",
    "p = new_model.predict(X_test_vect)\n",
    "acc = accuracy_score(p,y_test)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While the accuracy has decreased slightly in changing the model to an online capable format it is still accurate enough to continue with accuracy expected to approve as more information is passed to the model via the web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the entire data set to be used in the web application\n",
    "X_total = vect_optimized.transform(X)\n",
    "full_SGD = new_model.partial_fit(X_total,y,classes=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Model Serialization and function exportation for Website Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1) Pickle the model created and optimized above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle the model to be used in web application (along with stop words - hashing vector does not need to be pickled)\n",
    "\n",
    "dest = os.path.join('GlobalWarmingClassifier','website','pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "pickle.dump(full_SGD, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2) Define the vectorizing function and test the functunality of the pickle objects created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/zaneheald/Desktop/MachineLearning/zane_heald_machine_learning/machine-learning/projects/project02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zaneheald/Desktop/MachineLearning/zane_heald_machine_learning/machine-learning/projects/project02'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting GlobalWarmingClassifier/website/vectorizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile GlobalWarmingClassifier/website/vectorizer.py\n",
    "# Stand Alone Hashing Vector File\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(\n",
    "                os.path.join(cur_dir, \n",
    "                'pkl_objects', \n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    porter = PorterStemmer()\n",
    "    line = [porter.stem(word) for word in text.split()]\n",
    "    return line\n",
    "\n",
    "vect_optimized = HashingVectorizer(decode_error='ignore',\n",
    "                         norm = None,\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=preprocessor,\n",
    "                         stop_words = stop,\n",
    "                         tokenizer=tokenizer_porter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3) Test the functionality of the above exported functions and pickled models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('GlobalWarmingClassifier/website')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Probability: 99.97%\n"
     ]
    }
   ],
   "source": [
    "#check to ensure pickling worked properly\n",
    "\n",
    "from vectorizer import vect_optimized\n",
    "\n",
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))\n",
    "\n",
    "label = {0:'negative', 1:'positive'}\n",
    "example = [\"Global Warming is Fake News!\"]\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[clf.predict(X)[0]], \n",
    "       np.max(clf.predict_proba(X))*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above shows that the pickling has succesfully worked and predictions can be made regarding new 'tweets' passed to it. While the example above predicts incorrectly its purpose is to demonstrate that all the components for the web application are in place to handle future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Website Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the link to the website created for the final portion of this project which allows user creation of tweets that are predicted as either believing in climate change (posiitive) or disbelieving (negative). All data is recorded in an SQL lite data base for futher updates to the model created above:\n",
    "\n",
    "http://zaneheald.pythonanywhere.com/\n",
    "\n",
    "### Please see files included under the website folder for insite into the creation of the SQLite database, the updating of the pickle file, and HTML files used to create the website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Works Cited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to data world\n",
    "https://data.world/crowdflower/sentiment-of-climate-change/workspace/file?filename=1377884570_tweet_global_warming.csv\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
